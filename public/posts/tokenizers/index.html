<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>3476, 477, 12274, 112838, 248 | Andrea Gemelli</title>
<meta name="keywords" content="NLP, Large Langue Models, Tokenizers">
<meta name="description" content="Hint: &#34;deepseek-ai/DeepSeek-R1&#34; ü§ó">
<meta name="author" content="Andrea Gemelli">
<link rel="canonical" href="http://localhost:1313/posts/tokenizers/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.d991156f01fa690ee0abd2ae957ee466074e3679427ecc01d4d2e620f2066320.css" integrity="sha256-2ZEVbwH6aQ7gq9KulX7kZgdONnlCfswB1NLmIPIGYyA=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/assets/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/assets/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/assets/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/assets/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/assets/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/tokenizers/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="3476, 477, 12274, 112838, 248" />
<meta property="og:description" content="Hint: &#34;deepseek-ai/DeepSeek-R1&#34; ü§ó" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/posts/tokenizers/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-01-26T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-01-26T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="3476, 477, 12274, 112838, 248"/>
<meta name="twitter:description" content="Hint: &#34;deepseek-ai/DeepSeek-R1&#34; ü§ó"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://localhost:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "3476, 477, 12274, 112838, 248",
      "item": "http://localhost:1313/posts/tokenizers/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "3476, 477, 12274, 112838, 248",
  "name": "3476, 477, 12274, 112838, 248",
  "description": "Hint: \"deepseek-ai/DeepSeek-R1\" ü§ó",
  "keywords": [
    "NLP", "Large Langue Models", "Tokenizers"
  ],
  "articleBody": "Introduction When working with Large Language Models, we often focus on their remarkable capabilities - from writing code to explaining complex concepts. However, there‚Äôs a crucial component that can significantly impact their behavior and performance: tokenization üç£.\nAs highlighted in a recent work by Garreth Lee and the Hugging Face team ü§ó 1, even state-of-the-art models can stumble on seemingly simple tasks due to tokenization choices. For instance, many models struggle with the basic question ‚ÄúWhich is bigger? 9.9 or 9.11‚Äù - a failure that showcase their limitations in understanding how numbers are tokenized and represented internally.\nChatGPT, Claude and Phi struggling with simple questions.\nUnfortunately, sometimes models are not just struggling. Consider the infamous case of SolidGoldMagikarp - a string that, when input to the first ChatGPT, it could cause the model to produce nonsensical - or even harmful - outputs. This wasn‚Äôt a failure of the model‚Äôs understanding per se, but rather a reflection of an undertrained embedding vector accidentaly ended up in the vocabulary, consequently to the tokenization applied.\nIn these notes, I am going to review how tokenization works and which tokenizer library are available, to make informative decisions when developing language models.\nWhat are Tokenizers? As humans, we are born with an innate ability to acquire language and to understand its structure, which provides a blueprint for learning any language and generate new sentences to express ourselfs 2. Unfortunately, language ‚Äúas it is‚Äù cannot be digested by any architecture - just as images are processed as pixel values through neural networks! Tokenizers are the tools that implement a tokenization strategy, allowing to chunk the text into tokens and mapping each one into integers. Specifically:\nTokenization is the process of translating text into sequences of tokens - and vice versa - that produces a map between text and numbers; Tokens are what Karpathy calls the ‚Äúatoms of Transformers‚Äù 3, i.e. the fundamental blocks that build language models and make them able to generate meaningful words and sentences. Although the definitions are quite clear, the implementation of a tokenizer is far from simple. The strategy choice for tokens formation and vocabulary selection, will critically impact input encodings, model comprehension, and text generation capabilities.\nEmbedding the Text First of all, we need numbers and vectors to plug text into transformers and LLMs. How do we do so? ü§î\nThe naive solution would be to convert every character (letters, spaces, digits, symbols, emojis, etc.) into a unique integer. Following Python documentation, ‚Äústrings are immutable sequences of Unicode code points‚Äù, where Unicode is a standard definition of roughly 150k characters and 160 scripts, constantly evolving (UTF-8 preferred as described in this manifesto). In Python we can check the unicode of a character using the ord() function:\nprint(ord(\"A\")) \u003e\u003e 65 For a simple string as \"I love coding in Python ‚ù§Ô∏è\" a character by character tokenization would result in a vector of 25 characters:\ndef encode(text): return [ord(char) for char in text] print(encode(\"I love coding in Python ‚ù§Ô∏è\")) \u003e\u003e [73, 32, 108, 111, 118, 101, 32, 99, 111, 100, 105, 110, 103, 32, 105, 110, 32, 80, 121, 116, 104, 111, 110, 32, 10084, 65039] Great! We have our first encoding, but.. does this scale well? The answer is unfortunately no ü´†\nTransformer architectures are very high demanding architectures, being their inner product complexity O(N^2). This means that the computational cost of the model grows quadratically with the sequence length, suggesting that compressed inputs:\nwould be more computationally efficient; would include more text and context. This is where tokenizers come in. From the transformers library we can import thousands of different tokenizers. For example, let‚Äôs use the tokenizer of the well known DeepSeek-R14 architecture. The previous sentence would be transformed in:\nfrom transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1\") def encode(text): return tokenizer.encode(text) print(encode(\"I love coding in Python ‚ù§Ô∏è\")) \u003e\u003e [0, 43, 3518, 20255, 295, 15255, 53341, 100, 10759] We could reduced to 9 tokens the same sentence as before, cutting out ~65% of its previous lenght!\nLooking at its vocabulary, we can find out that:\n0 is a special token that marks the beginning of sentence \u003cÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú\u003e. 1 and 2 are reserved for the end of sentence and padding tokens, respectively. 43, 3518, 20255, 295, 15255 encode the sentence word by word; 53341, 100, 10759 are used for the emoji. Without looking at the file, you can directly decode the text from the list of integers:\ntokenizer.decode([0, 43, 3518, 20255, 295, 15255, 53341, 100, 10759]) \u003e\u003e \"I love coding in Python ‚ù§Ô∏è\" But who decides which tokens and numbers? Tokenization, of course!\nExisting Algorithms and Libraries There are several algorithms which split text into tokens and create a vocabulary. Among others, the most famous are:\nByte Pair Encoding 5 (BPE), iteratively merges frequent character pairs to build a vocabulary, starting from a base one of 256. It has been largely popularized by GPT 6 and more recently adopted by Llama3 7 and DeepSeek, to name a few; WordPiece 8 9 follows a similar approach to BPE but uses likelihood rather than frequency to select the merges. The vocabulary is optimized to maximize the informative aspect of the merges, not simply compressing the entire input text; Unigram (PAPER CITATION) uses probabilistic sampling to optimize a subword vocabulary by maximizing the likelihood of the training data. If you want to look more into the details of these approaches, the tokenizer summary made by HuggingFace it is perfect!\nLuckly, there are already open-source library that help you use the cited approaches without reimplementing them from scratch. The ones that I know, have used and suggests are the following:\nTokenizers (from HuggingFace ü§ó, link) highly customizable and trainable Implements BPE, WordPiece and Unigram! sentencepiece (from Google, link) It can do both training and inference It supports BPE and Unigram! tiktoken (from OpenAI, link) implements a fast BPE (3-6x faster than huggingface tokenizer) simpler compared to sentencepiece, but it allows only inference! MinBPE (from Karpathy ‚ù§Ô∏è, link) Unlike tiktoken, this code allows you to train your own BPE tokenizer. Training the RegexTokenizer on a large dataset (~100K), you would reproduce the GPT-4 tokenizer. Beyond traditional tokenizers Recently a new paper from Meta led the foundations for a new adaptive approach beyhond traditional tokenizers: The Byte Latent Transformer (BLT) 10.\nInstead of using predefined tokens, it proposes to dynamically groups bytes into patches based on their information content. Citing the paper, ‚Äúpatches are segmented dynamically based on the entropy of the next byte, allocating more compute and model capacity where there is more data complexity‚Äù. This approach shows promising results, with an efficiency gains of up to 50% and ‚Äúimproved robustness of BLT models to input noise and their awareness of sub-word aspects of input data that token-based llms miss‚Äù.\nBLT architecture.\nAs shown in the image, two lightweight local encoder / decoder are employed to transform bytes into patches dynamically, bypassing the need of a pre-trained fixed vocabulary. Than, the transformer architectures learns to predict the next patch given the input context directly within this embedded space.\nAn offial repository is already out, but to see its application at scale we still have to wait!\nConclusions Language models capabilities are boundend to, if not limited by, their plugged in tokenizers. The choice of the tokenization strategy and vocabulary size is as important as pre-training and allignment techinques. Key takeaways include:\nMisbeahviours on corner cases are usually highly due to tokenization; General tokenizers may require adjustments on specific downstream tasks to improve model capabilities, such as in math 1; The vocabulary size should be carefully chosen w.r.t. model training dimension to avoid under-trained tokens; Non-English languages may share common tokens with english, but special tokens should be added to consider multi-language applications 7; Emerging approaches like BLT suggests we might move soon beyond traditional tokenization; As we continue to improve language models, tokenization remains a critical area for innovation The recent success of approaches like BLT‚Äôs dynamic patching suggests that moving beyond fixed pre-trained vocabularies might be the key to build more efficient and capable models.\nReferences Lee, et al., ‚ÄúFrom Digits to Decisions: How Tokenization Impacts Arithmetic in LLMs‚Äù, 2024.¬†‚Ü©Ô∏é¬†‚Ü©Ô∏é\nChomsky, N, ‚ÄúAspects of the Theory of Syntax‚Äù, 1965¬†‚Ü©Ô∏é\nAndrej Karphaty, ‚ÄúLet‚Äôs build the GPT Tokenizer‚Äù, 2023¬†‚Ü©Ô∏é\nDeepSeek-AI, ‚ÄúDeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning‚Äù, GitHub, 2025¬†‚Ü©Ô∏é\nSennrich, et al., ‚ÄúNeural Machine Translation of Rare Words with Subword Units‚Äù, 2015¬†‚Ü©Ô∏é\nRadford, et al., ‚ÄúLanguage Models are Unsupervised Multitask Learners‚Äù, 2019¬†‚Ü©Ô∏é\nMeta AI, ‚ÄúThe Llama 3 Herd of Models‚Äù, 2024¬†‚Ü©Ô∏é¬†‚Ü©Ô∏é\nYonghui Wu et al., ‚ÄúGoogle‚Äôs neural machine translation system: Bridging the gap between human and machine translation.‚Äù, 2016¬†‚Ü©Ô∏é\nDevlin et al., ‚ÄúBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding‚Äù, 2019¬†‚Ü©Ô∏é\nPagnoni, et al, ‚ÄúByte Latent Transformer: Patches Scale Better Than Tokens‚Äù, 2024¬†‚Ü©Ô∏é\n",
  "wordCount" : "1458",
  "inLanguage": "en",
  "datePublished": "2025-01-26T00:00:00Z",
  "dateModified": "2025-01-26T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Andrea Gemelli"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/posts/tokenizers/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Andrea Gemelli",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/assets/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Andrea Gemelli (Alt + H)">Andrea Gemelli</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/about" title="About me">
                    <span>About me</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/publications" title="Publications">
                    <span>Publications</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/projects" title="Projects">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives" title="AI Notes">
                    <span>AI Notes</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      3476, 477, 12274, 112838, 248
    </h1>
    <div class="post-description">
      Hint: &#34;deepseek-ai/DeepSeek-R1&#34; ü§ó
    </div>
    <div class="post-meta"><span title='2025-01-26 00:00:00 +0000 UTC'>January 26, 2025</span>&nbsp;¬∑&nbsp;7 min&nbsp;¬∑&nbsp;Andrea Gemelli

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#what-are-tokenizers" aria-label="What are Tokenizers?">What are Tokenizers?</a><ul>
                        
                <li>
                    <a href="#embedding-the-text" aria-label="Embedding the Text">Embedding the Text</a></li>
                <li>
                    <a href="#existing-algorithms-and-libraries" aria-label="Existing Algorithms and Libraries">Existing Algorithms and Libraries</a></li>
                <li>
                    <a href="#beyond-traditional-tokenizers" aria-label="Beyond traditional tokenizers">Beyond traditional tokenizers</a></li></ul>
                </li>
                <li>
                    <a href="#conclusions" aria-label="Conclusions">Conclusions</a></li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>When working with Large Language Models, we often focus on their remarkable capabilities - from writing code to explaining complex concepts. However, there&rsquo;s a crucial component that can significantly impact their behavior and performance: tokenization üç£.</p>
<p>As highlighted in a recent work by Garreth Lee and the Hugging Face team ü§ó <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, even state-of-the-art models can stumble on seemingly simple tasks due to tokenization choices. For instance, many models struggle with the basic question &ldquo;Which is bigger? 9.9 or 9.11&rdquo; - a failure that showcase their limitations in understanding how numbers are tokenized and represented internally.</p>
<p><img loading="lazy" src="images/chats.png" alt="Models mistakes on &amp;ldquo;simple&amp;rdquo; questions"  />

<em>ChatGPT, Claude and Phi struggling with simple questions.</em></p>
<p>Unfortunately, sometimes models are not just <em>struggling</em>. Consider the infamous case of <a href="https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation">SolidGoldMagikarp</a> - a string that, when input to the first ChatGPT, it could cause the model to produce nonsensical - or even harmful - outputs. This wasn&rsquo;t a failure of the model&rsquo;s understanding <em>per se</em>, but rather a reflection of an undertrained embedding vector accidentaly ended up in the vocabulary, consequently to the tokenization applied.</p>
<p>In these notes, I am going to review how tokenization works and which tokenizer library are available, to make informative decisions when developing language models.</p>
<h2 id="what-are-tokenizers">What are Tokenizers?<a hidden class="anchor" aria-hidden="true" href="#what-are-tokenizers">#</a></h2>
<p>As humans, we are born with an innate ability to acquire language and to understand its structure, which provides a blueprint for learning <em>any</em> language and generate new sentences to express ourselfs <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.
Unfortunately, language <em>&ldquo;as it is&rdquo;</em> cannot be digested by any architecture - just as images are processed as pixel values through neural networks! Tokenizers are the tools that implement a tokenization strategy, allowing to chunk the text into tokens and mapping each one into integers. Specifically:</p>
<ul>
<li><strong>Tokenization</strong> is the process of translating text into sequences of tokens - and vice versa - that produces a map between text and numbers;</li>
<li><strong>Tokens</strong> are what Karpathy calls the &ldquo;atoms of Transformers&rdquo; <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>, i.e. the fundamental blocks that build language models and make them able to generate meaningful words and sentences.</li>
</ul>
<p>Although the definitions are quite clear, the implementation of a tokenizer is far from simple. The strategy choice for tokens formation and vocabulary selection, will critically impact input encodings, model comprehension, and text generation capabilities.</p>
<h3 id="embedding-the-text">Embedding the Text<a hidden class="anchor" aria-hidden="true" href="#embedding-the-text">#</a></h3>
<p>First of all, we need numbers and vectors to plug text into transformers and LLMs. How do we do so? ü§î</p>
<p>The naive solution would be to convert every character (letters, spaces, digits, symbols, emojis, etc.) into a unique integer. Following Python documentation, &ldquo;strings are immutable sequences of Unicode code points&rdquo;, where Unicode is a standard definition of roughly 150k characters and 160 scripts, constantly evolving (UTF-8 preferred as described in this <a href="http://utf8everywhere.org">manifesto</a>). In Python we can check the unicode of a character using the <code>ord()</code> function:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>print(ord(<span style="color:#e6db74">&#34;A&#34;</span>))
</span></span><span style="display:flex;"><span><span style="color:#f92672">&gt;&gt;</span> <span style="color:#ae81ff">65</span>
</span></span></code></pre></div><p>For a simple string as <code>&quot;I love coding in Python ‚ù§Ô∏è&quot;</code> a character by character tokenization would result in a vector of 25 characters:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">encode</span>(text):
</span></span><span style="display:flex;"><span>   <span style="color:#66d9ef">return</span> [ord(char) <span style="color:#66d9ef">for</span> char <span style="color:#f92672">in</span> text]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(encode(<span style="color:#e6db74">&#34;I love coding in Python ‚ù§Ô∏è&#34;</span>))
</span></span><span style="display:flex;"><span><span style="color:#f92672">&gt;&gt;</span> [<span style="color:#ae81ff">73</span>, <span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">108</span>, <span style="color:#ae81ff">111</span>, <span style="color:#ae81ff">118</span>, <span style="color:#ae81ff">101</span>, <span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">99</span>, <span style="color:#ae81ff">111</span>, <span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">105</span>, <span style="color:#ae81ff">110</span>, <span style="color:#ae81ff">103</span>, <span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">105</span>, <span style="color:#ae81ff">110</span>, <span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">80</span>, <span style="color:#ae81ff">121</span>, <span style="color:#ae81ff">116</span>, <span style="color:#ae81ff">104</span>, <span style="color:#ae81ff">111</span>, <span style="color:#ae81ff">110</span>, <span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">10084</span>, <span style="color:#ae81ff">65039</span>]
</span></span></code></pre></div><p>Great! We have our first encoding, but.. does this scale well? The answer is unfortunately no ü´†</p>
<p>Transformer architectures are very high demanding architectures, being their inner product complexity <code>O(N^2)</code>. This means that the computational cost of the model grows quadratically with the sequence length, suggesting that compressed inputs:</p>
<ul>
<li>would be more computationally efficient;</li>
<li>would include more text and context.</li>
</ul>
<p>This is where tokenizers come in.
From the <a href="https://huggingface.co/docs/transformers/index">transformers library</a> we can import thousands of different tokenizers.
For example, let&rsquo;s use the tokenizer of the well known DeepSeek-R1<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> architecture. The previous sentence would be transformed in:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> AutoTokenizer
</span></span><span style="display:flex;"><span>tokenizer <span style="color:#f92672">=</span> AutoTokenizer<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#34;deepseek-ai/DeepSeek-R1&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">encode</span>(text):
</span></span><span style="display:flex;"><span>   <span style="color:#66d9ef">return</span> tokenizer<span style="color:#f92672">.</span>encode(text)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(encode(<span style="color:#e6db74">&#34;I love coding in Python ‚ù§Ô∏è&#34;</span>))
</span></span><span style="display:flex;"><span><span style="color:#f92672">&gt;&gt;</span> [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">43</span>, <span style="color:#ae81ff">3518</span>, <span style="color:#ae81ff">20255</span>, <span style="color:#ae81ff">295</span>, <span style="color:#ae81ff">15255</span>, <span style="color:#ae81ff">53341</span>, <span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">10759</span>]
</span></span></code></pre></div><p>We could reduced to 9 tokens the same sentence as before, cutting out ~65% of its previous lenght!</p>
<p>Looking at <a href="https://huggingface.co/deepseek-ai/DeepSeek-R1/raw/main/tokenizer.json">its vocabulary</a>, we can find out that:</p>
<ul>
<li><code>0</code> is a special token that marks the beginning of sentence <code>&lt;ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú&gt;</code>. 1 and 2 are reserved for the end of sentence and padding tokens, respectively.</li>
<li><code>43, 3518, 20255, 295, 15255</code> encode the sentence word by word;</li>
<li><code>53341, 100, 10759</code> are used for the emoji.</li>
</ul>
<p>Without looking at the file, you can directly decode the text from the list of integers:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>tokenizer<span style="color:#f92672">.</span>decode([<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">43</span>, <span style="color:#ae81ff">3518</span>, <span style="color:#ae81ff">20255</span>, <span style="color:#ae81ff">295</span>, <span style="color:#ae81ff">15255</span>, <span style="color:#ae81ff">53341</span>, <span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">10759</span>])
</span></span><span style="display:flex;"><span><span style="color:#f92672">&gt;&gt;</span> <span style="color:#e6db74">&#34;I love coding in Python ‚ù§Ô∏è&#34;</span>
</span></span></code></pre></div><p>But who decides which tokens and numbers? Tokenization, of course!</p>
<h3 id="existing-algorithms-and-libraries">Existing Algorithms and Libraries<a hidden class="anchor" aria-hidden="true" href="#existing-algorithms-and-libraries">#</a></h3>
<p>There are several algorithms which split text into tokens and create a vocabulary.
Among others, the most famous are:</p>
<ol>
<li><strong>Byte Pair Encoding</strong> <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> (BPE), iteratively merges frequent character pairs to build a vocabulary, starting from a base one of 256. It has been largely popularized by GPT <sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> and more recently adopted by Llama3 <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup> and DeepSeek, to name a few;</li>
<li><strong>WordPiece</strong> <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup> <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>  follows a similar approach to BPE but uses likelihood rather than frequency to select the merges. The vocabulary is optimized to maximize the informative aspect of the merges, not simply compressing the entire input text;</li>
<li><strong>Unigram</strong> (PAPER CITATION) uses probabilistic sampling to optimize a subword vocabulary by maximizing the likelihood of the training data.</li>
</ol>
<p>If you want to look more into the details of these approaches, the <a href="https://huggingface.co/docs/transformers/tokenizer_summary">tokenizer summary</a> made by HuggingFace it is perfect!</p>
<p>Luckly, there are already open-source library that help you use the cited approaches without reimplementing them from scratch.
The ones that I know, have used and suggests are the following:</p>
<ul>
<li><strong>Tokenizers</strong> (from HuggingFace ü§ó, <a href="https://github.com/huggingface/tokenizers">link</a>)
<ul>
<li>highly customizable and trainable</li>
<li>Implements BPE, WordPiece and Unigram!</li>
</ul>
</li>
<li><strong>sentencepiece</strong> (from Google, <a href="https://github.com/google/sentencepiece">link</a>)
<ul>
<li>It can do both training and inference</li>
<li>It supports BPE and Unigram!</li>
</ul>
</li>
<li><strong>tiktoken</strong> (from OpenAI, <a href="https://github.com/openai/tiktoken/tree/main">link</a>)
<ul>
<li>implements a fast BPE (3-6x faster than huggingface tokenizer)</li>
<li>simpler compared to sentencepiece, but it allows only inference!</li>
</ul>
</li>
<li><strong>MinBPE</strong> (from Karpathy ‚ù§Ô∏è, <a href="https://github.com/karpathy/minbpe">link</a>)
<ul>
<li>Unlike tiktoken, this code allows you to train your own BPE tokenizer.</li>
<li>Training the RegexTokenizer on a large dataset (~100K), you would reproduce the GPT-4 tokenizer.</li>
</ul>
</li>
</ul>
<h3 id="beyond-traditional-tokenizers">Beyond traditional tokenizers<a hidden class="anchor" aria-hidden="true" href="#beyond-traditional-tokenizers">#</a></h3>
<p>Recently a new paper from Meta led the foundations for a new adaptive approach beyhond traditional tokenizers: The Byte Latent Transformer (BLT) <sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>.</p>
<p>Instead of using predefined tokens, it proposes to dynamically groups bytes into patches based on their information content. Citing the paper, &ldquo;patches are segmented dynamically based on the entropy of the next byte, allocating more compute and model capacity where there is more data complexity&rdquo;. This approach shows promising results, with an efficiency gains of up to 50% and &ldquo;improved robustness of BLT models to input noise and their awareness of sub-word aspects of input data that token-based llms miss&rdquo;.</p>
<p><img loading="lazy" src="images/blt-arch.png" alt="Models mistakes on &amp;ldquo;simple&amp;rdquo; questions"  />

<em>BLT architecture.</em></p>
<p>As shown in the image, two lightweight local encoder / decoder are employed to transform bytes into patches dynamically, bypassing the need of a pre-trained fixed vocabulary. Than, the transformer architectures learns to predict the next <em>patch</em> given the input context directly within this embedded space.</p>
<p>An <a href="https://github.com/facebookresearch/blt">offial repository</a> is already out, but to see its application at scale we still have to wait!</p>
<h2 id="conclusions">Conclusions<a hidden class="anchor" aria-hidden="true" href="#conclusions">#</a></h2>
<p>Language models capabilities are boundend to, if not limited by, their plugged in tokenizers.
The choice of the tokenization strategy and vocabulary size is as important as pre-training and allignment techinques. Key takeaways include:</p>
<ul>
<li>Misbeahviours on corner cases are usually highly due to tokenization;</li>
<li>General tokenizers may require adjustments on specific downstream tasks to improve model capabilities, such as in math <sup id="fnref1:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>;</li>
<li>The vocabulary size should be carefully chosen w.r.t. model training dimension to avoid under-trained tokens;</li>
<li>Non-English languages may share common tokens with english, but special tokens should be added to consider multi-language applications <sup id="fnref1:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>;</li>
<li>Emerging approaches like BLT suggests we might move soon beyond traditional tokenization;</li>
</ul>
<p>As we continue to improve language models, tokenization remains a critical area for innovation
The recent success of approaches like BLT&rsquo;s dynamic patching suggests that moving beyond fixed pre-trained vocabularies might be the key to build more efficient and capable models.</p>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Lee, et al., &ldquo;From Digits to Decisions: How Tokenization Impacts Arithmetic in LLMs&rdquo;, 2024.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Chomsky, N, <em>&ldquo;Aspects of the Theory of Syntax&rdquo;</em>, 1965&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Andrej Karphaty, <a href="https://www.youtube.com/watch?v=zduSFxRajkE">&ldquo;Let&rsquo;s build the GPT Tokenizer&rdquo;</a>, 2023&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>DeepSeek-AI, <em>&ldquo;DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning&rdquo;</em>, <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf">GitHub</a>, 2025&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Sennrich, et al., <em>&ldquo;Neural Machine Translation of Rare Words with Subword Units&rdquo;</em>, 2015&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Radford, et al., <em>&ldquo;Language Models are Unsupervised Multitask Learners&rdquo;</em>, 2019&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Meta AI, <em>&ldquo;The Llama 3 Herd of Models&rdquo;</em>, 2024&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a>&#160;<a href="#fnref1:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Yonghui Wu et al., <em>&ldquo;Google‚Äôs neural machine translation system: Bridging the gap between human and machine translation.&rdquo;</em>, 2016&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Devlin et al., <em>&ldquo;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&rdquo;</em>, 2019&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Pagnoni, et al, &ldquo;Byte Latent Transformer: Patches Scale Better Than Tokens&rdquo;, 2024&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/nlp/">NLP</a></li>
      <li><a href="http://localhost:1313/tags/large-langue-models/">Large Langue Models</a></li>
      <li><a href="http://localhost:1313/tags/tokenizers/">Tokenizers</a></li>
    </ul>

<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 3476, 477, 12274, 112838, 248 on x"
            href="https://x.com/intent/tweet/?text=3476%2c%20477%2c%2012274%2c%20112838%2c%20248&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2ftokenizers%2f&amp;hashtags=NLP%2cLargeLangueModels%2cTokenizers">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 3476, 477, 12274, 112838, 248 on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2ftokenizers%2f&amp;title=3476%2c%20477%2c%2012274%2c%20112838%2c%20248&amp;summary=3476%2c%20477%2c%2012274%2c%20112838%2c%20248&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2ftokenizers%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
</ul>

  </footer><script src="https://giscus.app/client.js"
        data-repo="andreagemelli/andreagemelli.github.io"
        data-repo-id="R_kgDOH26OFQ"
        data-category="Q&A"
        data-category-id="DIC_kwDOH26OFc4CkUt5"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="preferred_color_scheme"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">Andrea Gemelli</a></span> ¬∑ 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
