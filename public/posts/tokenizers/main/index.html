<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>[0, 3476, 477, 12274, 112838, 248] | Andrea Gemelli</title>
<meta name="keywords" content="NLP, Large Langue Models, Tokenizers">
<meta name="description" content="Tokenizers - The Building Blocks of Language Models">
<meta name="author" content="Andrea Gemelli">
<link rel="canonical" href="http://localhost:1313/posts/tokenizers/main/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.54f6d3849888d9ca90430bf3a058740e423abebb611e6332d18bf13a653ec3aa.css" integrity="sha256-VPbThJiI2cqQQwvzoFh0DkI6vrthHmMy0YvxOmU&#43;w6o=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/assets/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/assets/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/assets/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/assets/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/assets/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/tokenizers/main/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="[0, 3476, 477, 12274, 112838, 248]" />
<meta property="og:description" content="Tokenizers - The Building Blocks of Language Models" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/posts/tokenizers/main/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-01-26T00:00:00+00:00" />
<meta property="article:modified_time" content="2025-01-26T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="[0, 3476, 477, 12274, 112838, 248]"/>
<meta name="twitter:description" content="Tokenizers - The Building Blocks of Language Models"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://localhost:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "[0, 3476, 477, 12274, 112838, 248]",
      "item": "http://localhost:1313/posts/tokenizers/main/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "[0, 3476, 477, 12274, 112838, 248]",
  "name": "[0, 3476, 477, 12274, 112838, 248]",
  "description": "Tokenizers - The Building Blocks of Language Models",
  "keywords": [
    "NLP", "Large Langue Models", "Tokenizers"
  ],
  "articleBody": "Introduction When working with Large Language Models, we often focus on their remarkable capabilities - from writing code to explaining complex concepts. However, there‚Äôs a crucial component that can significantly impact their behavior and performance: tokenization.\nAs highlighted in a recent work by Garreth Lee and the Hugging Face team team ü§ó 1, even state-of-the-art models can stumble on seemingly simple tasks due to tokenization choices. For instance, many models struggle with a basic question ‚ÄúWhich is bigger? 9.9 or 9.11‚Äù - a failure that stems not from their reasoning capabilities, but from how numbers are tokenized and represented internally.\nConsider also the infamous case of ‚ÄúSolidGoldMagikarp‚Äù - a string that, when input to the first ChatGPT, it could cause the model to produce nonsensical outputs. This wasn‚Äôt a failure of the model‚Äôs understanding but rather an artifact of how the text was broken down into tokens, demonstrating how tokenization directly influences model behavior.\nWhat are Tokenizers? As humans, we are born with an innate ability to acquire language and to understand its structure, which provides a blueprint for learning any language and generate new sentences to express ourselfs 2. Unfortunately, language ‚Äúas it is‚Äù cannot be digested by any architecture - just as images are processed as pixel values through neural networks!\nTokenization is the process of translating text into sequences of tokens, and vice versa, a map needed by Natural Language Processing application between text and numbers. The elements that bridge this gap between the two ‚Äúworlds‚Äù are tokens.\nTokens are what Karpathy calls the ‚Äúatoms of Transformers‚Äù 3, but most importantly, they are the fundamental blocks that build language to form meaningful words and sentences. What changes significantly about a tokenizer is how tokens are considered: what constitutes a token (a character? a group of characters? a word?) and which chunks of text can be considered a token?\nEmbedding the Text We need numbers and vectors to plug text into transformers and LLMs. How do we do so? The naive solution would be to convert every character (letters, spaces, digits, symbols, emojis, etc.) into a unique integer. Following Python documentation, ‚Äústrings are immutable sequences of Unicode code points‚Äù, where Unicode is a standard definition of roughly 150k characters and 160 scripts, constantly evolving (UTF-8 preferred as described in this !manifesto).\n(dont know about this part ‚Ä¶.)In Python we can check the unicode of a character using the ord() function:\nord(\"A\") = 65 ADDING HERE EXAMPLES from TIKTOKENIZERS\nExisting Algorithms There are several algorithms which split text into tokens and create a vocabulary. Among the most famous are:\nByte Pair Encoding (BPE) (MISSING GPT PAPER CITATION) WordPiece (by Google, BERT) (PAPER CITATION) Unigram (PAPER CITATION) The Byte Pair Encoding (BPE) Directly using character encoding would result in very long sequences, which we would like to compress for optimization reasons within attention layers. This is where Byte Pair Encoding comes in, enlarging the vocabulary to compress together the most frequent repeated (thus important/informative?) subsequences.\nThe tradeoff between sequence mean length and vocabulary size is arbitrary and based on heuristic/experience. In the BPE algorithm, the number of merges to be performed will be the vocabulary size desired minus the ‚Äústandard‚Äù from the unicode one.\nNotably:\nGPT-2 used a 52k vocabulary The newest GPT models use 100k and 200k versions Llama uses a slightly variant of BPE 100k with +28k tokens An important consideration is that non-English text is ‚Äústretched-out‚Äù and more likely to run out of context: the tokenizer is trained primarily on English text, making it more likely to find pieces of English text that can be joined together, while other languages must often be encoded in smaller chunks.\nWordPiece (WP) The initial alphabet contains all characters present in the pretraining text: the leading characters and subsequent characters preceded by the ‚Äò#‚Äô prefix. For example, the word ‚Äòhello‚Äô becomes [‚Äòh‚Äô, ‚Äò##e‚Äô, ‚Äò##l‚Äô, ‚Äò##l‚Äô, ‚Äò##o‚Äô].\nLike BPE, WordPiece learns merge rules but computes a score for each pair using a specific formula that considers frequencies. Even if a pair is very frequent, if it is composed of very frequent sub-terms, it is not merged. This way, the vocabulary is optimized to maximize the informative aspect of the merges, not simply compressing the entire input text.\nThe Evolution of Tokenization Recent developments have shown interesting approaches to improving tokenization:\nLlama 3 introduced three-digit tokenization for numbers, striking a balance between token efficiency and maintaining consistent representation. By grouping numbers into tokens of up to three digits (e.g., 123456 ‚Üí [123, 456]), it reduced token counts while maintaining precision for math-heavy tasks.\nThe Byte Latent Transformer (BLT) proposes a novel approach that breaks free from fixed vocabularies altogether. Instead of using predefined tokens, it dynamically groups bytes into patches based on their information content. This approach shows promising results in:\nImproved efficiency in processing data Better handling of non-English languages More robust performance on long-tail sequences MISSING Existing libraries from letxbe documentation Conclusions The choice of tokenizer and vocabulary size is as important as the architecture and its training. Key takeaways include:\nDifferent tokenizers offer different tradeoffs between efficiency and accuracy The vocabulary size should be carefully chosen to avoid under-trained tokens Non-English languages often require special consideration Emerging approaches like BLT suggest we might move beyond traditional tokenization As we continue to scale language models, tokenization remains a critical area for innovation. The recent success of approaches like BLT‚Äôs dynamic patching suggests that moving beyond fixed vocabularies might be key to building more efficient and capable models.\nReferences Lee, et al., ‚ÄúFrom Digits to Decisions: How Tokenization Impacts Arithmetic in LLMs‚Äù, 2024.¬†‚Ü©Ô∏é\nChomsky, N, ‚ÄúAspects of the Theory of Syntax‚Äù, 1965¬†‚Ü©Ô∏é\nAndrej Karphaty, ‚ÄúLet‚Äôs build the GPT Tokenizer‚Äù, 2023¬†‚Ü©Ô∏é\n",
  "wordCount" : "948",
  "inLanguage": "en",
  "datePublished": "2025-01-26T00:00:00Z",
  "dateModified": "2025-01-26T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Andrea Gemelli"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/posts/tokenizers/main/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Andrea Gemelli",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/assets/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Andrea Gemelli (Alt + H)">Andrea Gemelli</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/about" title="About me">
                    <span>About me</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/publications" title="Publications">
                    <span>Publications</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/projects" title="Projects">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives" title="AI Notes">
                    <span>AI Notes</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      [0, 3476, 477, 12274, 112838, 248]
    </h1>
    <div class="post-description">
      Tokenizers - The Building Blocks of Language Models
    </div>
    <div class="post-meta"><span title='2025-01-26 00:00:00 +0000 UTC'>January 26, 2025</span>&nbsp;¬∑&nbsp;5 min&nbsp;¬∑&nbsp;Andrea Gemelli

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a></li>
                <li>
                    <a href="#what-are-tokenizers" aria-label="What are Tokenizers?">What are Tokenizers?</a></li>
                <li>
                    <a href="#embedding-the-text" aria-label="Embedding the Text">Embedding the Text</a></li>
                <li>
                    <a href="#existing-algorithms" aria-label="Existing Algorithms">Existing Algorithms</a><ul>
                        
                <li>
                    <a href="#the-byte-pair-encoding-bpe" aria-label="The Byte Pair Encoding (BPE)">The Byte Pair Encoding (BPE)</a></li>
                <li>
                    <a href="#wordpiece-wp" aria-label="WordPiece (WP)">WordPiece (WP)</a></li></ul>
                </li>
                <li>
                    <a href="#the-evolution-of-tokenization" aria-label="The Evolution of Tokenization">The Evolution of Tokenization</a></li>
                <li>
                    <a href="#missing-existing-libraries-from-letxbe-documentation" aria-label="MISSING Existing libraries from letxbe documentation">MISSING Existing libraries from letxbe documentation</a></li>
                <li>
                    <a href="#conclusions" aria-label="Conclusions">Conclusions</a></li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>When working with Large Language Models, we often focus on their remarkable capabilities - from writing code to explaining complex concepts. However, there&rsquo;s a crucial component that can significantly impact their behavior and performance: tokenization.</p>
<p>As highlighted in a recent work by Garreth Lee and the Hugging Face team team ü§ó <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, even state-of-the-art models can stumble on seemingly simple tasks due to tokenization choices. For instance, many models struggle with a basic question &ldquo;Which is bigger? 9.9 or 9.11&rdquo; - a failure that stems not from their reasoning capabilities, but from how numbers are tokenized and represented internally.</p>
<p><img loading="lazy" src="images/chats.png" alt="Models mistakes on &amp;ldquo;simple&amp;rdquo; questions"  />
</p>
<p>Consider also the infamous case of <a href="https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation">&ldquo;SolidGoldMagikarp&rdquo;</a> - a string that, when input to the first ChatGPT, it could cause the model to produce nonsensical outputs. This wasn&rsquo;t a failure of the model&rsquo;s understanding but rather an artifact of how the text was broken down into tokens, demonstrating how tokenization directly influences model behavior.</p>
<h2 id="what-are-tokenizers">What are Tokenizers?<a hidden class="anchor" aria-hidden="true" href="#what-are-tokenizers">#</a></h2>
<p>As humans, we are born with an innate ability to acquire language and to understand its structure, which provides a blueprint for learning <em>any</em> language and generate new sentences to express ourselfs <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.
Unfortunately, language &ldquo;as it is&rdquo; cannot be digested by any architecture - just as images are processed as pixel values through neural networks!</p>
<p>Tokenization is the process of translating text into sequences of tokens, and vice versa, a map needed by Natural Language Processing application between text and numbers. The elements that bridge this gap between the two &ldquo;worlds&rdquo; are tokens.</p>
<p>Tokens are what Karpathy calls the &ldquo;atoms of Transformers&rdquo; <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>, but most importantly, they are the fundamental blocks that build language to form meaningful words and sentences.
What changes significantly about a tokenizer is how tokens are considered: what constitutes a token (a character? a group of characters? a word?) and which chunks of text can be considered a token?</p>
<h2 id="embedding-the-text">Embedding the Text<a hidden class="anchor" aria-hidden="true" href="#embedding-the-text">#</a></h2>
<p>We need numbers and vectors to plug text into transformers and LLMs. How do we do so? The naive solution would be to convert every character (letters, spaces, digits, symbols, emojis, etc.) into a unique integer. Following Python documentation, &ldquo;strings are immutable sequences of Unicode code points&rdquo;, where Unicode is a standard definition of roughly 150k characters and 160 scripts, constantly evolving (UTF-8 preferred as described in this <a href="http://utf8everywhere.org">!manifesto</a>).</p>
<p>(dont know about this part &hellip;.)In Python we can check the unicode of a character using the <code>ord()</code> function:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>ord(<span style="color:#e6db74">&#34;A&#34;</span>) <span style="color:#f92672">=</span> <span style="color:#ae81ff">65</span>
</span></span></code></pre></div><p>ADDING HERE EXAMPLES from TIKTOKENIZERS</p>
<h2 id="existing-algorithms">Existing Algorithms<a hidden class="anchor" aria-hidden="true" href="#existing-algorithms">#</a></h2>
<p>There are several algorithms which split text into tokens and create a vocabulary. Among the most famous are:</p>
<ol>
<li>Byte Pair Encoding (BPE) (MISSING GPT PAPER CITATION)</li>
<li>WordPiece (by Google, BERT) (PAPER CITATION)</li>
<li>Unigram (PAPER CITATION)</li>
</ol>
<h3 id="the-byte-pair-encoding-bpe">The Byte Pair Encoding (BPE)<a hidden class="anchor" aria-hidden="true" href="#the-byte-pair-encoding-bpe">#</a></h3>
<p>Directly using character encoding would result in very long sequences, which we would like to compress for optimization reasons within attention layers. This is where Byte Pair Encoding comes in, enlarging the vocabulary to compress together the most frequent repeated (thus important/informative?) subsequences.</p>
<p>The tradeoff between sequence mean length and vocabulary size is arbitrary and based on heuristic/experience. In the BPE algorithm, the number of merges to be performed will be the vocabulary size desired minus the &ldquo;standard&rdquo; from the unicode one.</p>
<p>Notably:</p>
<ul>
<li>GPT-2 used a 52k vocabulary</li>
<li>The newest GPT models use 100k and 200k versions</li>
<li>Llama uses a slightly variant of BPE 100k with +28k tokens</li>
</ul>
<p>An important consideration is that non-English text is &ldquo;stretched-out&rdquo; and more likely to run out of context: the tokenizer is trained primarily on English text, making it more likely to find pieces of English text that can be joined together, while other languages must often be encoded in smaller chunks.</p>
<h3 id="wordpiece-wp">WordPiece (WP)<a hidden class="anchor" aria-hidden="true" href="#wordpiece-wp">#</a></h3>
<p>The initial alphabet contains all characters present in the pretraining text: the leading characters and subsequent characters preceded by the &lsquo;#&rsquo; prefix. For example, the word &lsquo;hello&rsquo; becomes [&lsquo;h&rsquo;, &lsquo;##e&rsquo;, &lsquo;##l&rsquo;, &lsquo;##l&rsquo;, &lsquo;##o&rsquo;].</p>
<p>Like BPE, WordPiece learns merge rules but computes a score for each pair using a specific formula that considers frequencies. Even if a pair is very frequent, if it is composed of very frequent sub-terms, it is not merged. This way, the vocabulary is optimized to maximize the informative aspect of the merges, not simply compressing the entire input text.</p>
<h2 id="the-evolution-of-tokenization">The Evolution of Tokenization<a hidden class="anchor" aria-hidden="true" href="#the-evolution-of-tokenization">#</a></h2>
<p>Recent developments have shown interesting approaches to improving tokenization:</p>
<ol>
<li>
<p>Llama 3 introduced three-digit tokenization for numbers, striking a balance between token efficiency and maintaining consistent representation. By grouping numbers into tokens of up to three digits (e.g., 123456 ‚Üí [123, 456]), it reduced token counts while maintaining precision for math-heavy tasks.</p>
</li>
<li>
<p>The Byte Latent Transformer (BLT) proposes a novel approach that breaks free from fixed vocabularies altogether. Instead of using predefined tokens, it dynamically groups bytes into patches based on their information content. This approach shows promising results in:</p>
<ul>
<li>Improved efficiency in processing data</li>
<li>Better handling of non-English languages</li>
<li>More robust performance on long-tail sequences</li>
</ul>
</li>
</ol>
<h2 id="missing-existing-libraries-from-letxbe-documentation">MISSING Existing libraries from letxbe documentation<a hidden class="anchor" aria-hidden="true" href="#missing-existing-libraries-from-letxbe-documentation">#</a></h2>
<h2 id="conclusions">Conclusions<a hidden class="anchor" aria-hidden="true" href="#conclusions">#</a></h2>
<p>The choice of tokenizer and vocabulary size is as important as the architecture and its training. Key takeaways include:</p>
<ul>
<li>Different tokenizers offer different tradeoffs between efficiency and accuracy</li>
<li>The vocabulary size should be carefully chosen to avoid under-trained tokens</li>
<li>Non-English languages often require special consideration</li>
<li>Emerging approaches like BLT suggest we might move beyond traditional tokenization</li>
</ul>
<p>As we continue to scale language models, tokenization remains a critical area for innovation.
The recent success of approaches like BLT&rsquo;s dynamic patching suggests that moving beyond fixed vocabularies might be key to building more efficient and capable models.</p>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Lee, et al., &ldquo;From Digits to Decisions: How Tokenization Impacts Arithmetic in LLMs&rdquo;, 2024.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Chomsky, N, <em>&ldquo;Aspects of the Theory of Syntax&rdquo;</em>, 1965&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Andrej Karphaty, <a href="https://www.youtube.com/watch?v=zduSFxRajkE">&ldquo;Let&rsquo;s build the GPT Tokenizer&rdquo;</a>, 2023&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/nlp/">NLP</a></li>
      <li><a href="http://localhost:1313/tags/large-langue-models/">Large Langue Models</a></li>
      <li><a href="http://localhost:1313/tags/tokenizers/">Tokenizers</a></li>
    </ul>

<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share [0, 3476, 477, 12274, 112838, 248] on x"
            href="https://x.com/intent/tweet/?text=%5b0%2c%203476%2c%20477%2c%2012274%2c%20112838%2c%20248%5d&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2ftokenizers%2fmain%2f&amp;hashtags=NLP%2cLargeLangueModels%2cTokenizers">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share [0, 3476, 477, 12274, 112838, 248] on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2ftokenizers%2fmain%2f&amp;title=%5b0%2c%203476%2c%20477%2c%2012274%2c%20112838%2c%20248%5d&amp;summary=%5b0%2c%203476%2c%20477%2c%2012274%2c%20112838%2c%20248%5d&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2ftokenizers%2fmain%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
</ul>

  </footer><script src="https://giscus.app/client.js"
        data-repo="andreagemelli/andreagemelli.github.io"
        data-repo-id="R_kgDOH26OFQ"
        data-category="Q&A"
        data-category-id="DIC_kwDOH26OFc4CkUt5"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="preferred_color_scheme"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">Andrea Gemelli</a></span> ¬∑ 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
